INSERT INTO `page` (`pageid`, `uri`, `indexedat`, `content`) VALUES
(1, 'https://en.wikipedia.org/wiki/FreeNATS', '2019-12-03 13:45:02', 'FreeNATS\r\nOriginal author(s)    David Cutting\r\nDeveloper(s)    PurplePixie Systems\r\nInitial release    March 2, 2008[1]\r\nStable release\r\n1.20.1b / November 14, 2018; 12 months ago[2]\r\nOperating system    Unix-like\r\nPlatform    PHP / MySQL\r\nAvailable in    English\r\nType    Network monitoring\r\nLicense    GNU General Public License\r\nWebsite    www.purplepixie.org/freenats/\r\nFreeNATS (the Free Network Automatic Testing System) is an open-source network monitoring software application[3] developed by David Cutting under the banner of PurplePixie Systems.\r\n\r\nFreeNATS is free software licensed under the terms of the GNU General Public License version 3 as published by the Free Software Foundation.\r\n\r\n\r\nOverview\r\nMonitoring of network services (SMTP, POP3, HTTP, ICMP (ping)[4]\r\nLimited monitoring of host resources (processor load, disk usage) on a majority of network operating systems, including Microsoft Windows and Linux through agent-based testing\r\nPlugin design that allows users to easily develop their own service checks depending on needs, by using PHP (or other languages or scripts wrapped in PHP)\r\nSome ability to define network host hierarchy using \"master\" nodes allowing link failures to suspend monitoring[5]\r\nEvent-based system allowing failure notifications to be sent in customised email (suitable for email-to-SMS) or to utilise third-party notification scripts via a plug-in\r\nAbility to define event handlers to be run during service or host events for proactive problem resolution\r\nAutomatic data retention cleanups\r\nFull web-interface for management and monitoring\r\nAbility to \"publish\" views and graphs within third-party web pages'),
(2, 'https://en.wikipedia.org/wiki/Network_monitoring', '2019-12-03 13:45:02', 'Network monitoring\r\nFrom Wikipedia, the free encyclopedia\r\n  (Redirected from Network monitoring system)\r\nJump to navigationJump to search\r\nThis article is about monitoring for technical failures. For monitoring in the sense of surveillance, but relying mostly on the same technology, see Computer and network surveillance.\r\nNetwork monitoring is the use of a system that constantly monitors a computer network for slow or failing components and that notifies the network administrator (via email, SMS or other alarms) in case of outages or other trouble. Network monitoring is part of network management.\r\n\r\n\r\nContents\r\n1	Details\r\n2	Network tomography\r\n3	Route analytics\r\n4	Various types of protocols\r\n5	Internet server monitoring\r\n5.1	Servers around the globe\r\n5.2	Web server monitoring process\r\n5.3	Notification\r\n6	See also\r\n7	Notes and references\r\n8	External links\r\nDetails\r\nWhile an intrusion detection system monitors a network for threats from the outside, a network monitoring system monitors the network for problems caused by overloaded or crashed servers, network connections or other devices.\r\n\r\nFor example, to determine the status of a web server, monitoring software may periodically send an HTTP request to fetch a page. For email servers, a test message might be sent through SMTP and retrieved by IMAP or POP3.\r\n\r\nCommonly measured metrics are response time, availability and uptime, although both consistency and reliability metrics are starting to gain popularity. The widespread addition of WAN optimization devices is having an adverse effect on most network monitoring tools, especially when it comes to measuring accurate end-to-end delay because they limit round-trip delay time visibility.[1]\r\n\r\nStatus request failures, such as when a connection cannot be established, it times-out, or the document or message cannot be retrieved, usually produce an action from the monitoring system. These actions vary; An alarm may be sent (via SMS, email, etc.) to the resident sysadmin, automatic failover systems may be activated to remove the troubled server from duty until it can be repaired, etc.\r\n\r\nMonitoring the performance of a network uplink is also known as network traffic measurement.\r\n\r\nNetwork tomography\r\nNetwork tomography is an important area of network measurement, which deals with monitoring the health of various links in a network using end-to-end probes sent by agents located at vantage points in the network/Internet.\r\n\r\nRoute analytics\r\nRoute analytics is another important area of network measurement. It includes the methods, systems, algorithms and tools to monitor the routing posture of networks. Incorrect routing or routing issues cause undesirable performance degradation or downtime.\r\n\r\nVarious types of protocols\r\nSite monitoring services can check HTTP pages, HTTPS, SNMP, FTP, SMTP, POP3, IMAP, DNS, SSH, TELNET, SSL, TCP, ICMP, SIP, UDP, Media Streaming and a range of other ports with a variety of check intervals ranging from every four hours to every one minute. Typically, most network monitoring services test your server anywhere between once-per-hour to once-per-minute.\r\n\r\nInternet server monitoring\r\nSee also: Website monitoring\r\nMonitoring an internet server means that the server owner always knows if one or all of his services go down. Server monitoring may be internal, i.e. web server software checks its status and notifies the owner if some services go down, and external, i.e. some web server monitoring companies check the services status with a certain frequency. Server monitoring can encompass a check of system metrics, such as CPU usage, memory usage, network performance and disk space. It can also include application monitoring, such as checking the processes of programs such as Apache, MySQL, Nginx, Postgres and others.\r\n\r\nExternal monitoring is more reliable, as it keeps on working when the server completely goes down. Good server monitoring tools also have performance benchmarking, alerting capabilities and the ability to link certain thresholds with automated server jobs, such as provisioning more memory or performing a backup.\r\n\r\nServers around the globe\r\nNetwork monitoring services usually have a number of servers around the globe - for example in America, Europe, Asia, Australia and other locations. By having multiple servers in different geographic locations, a monitoring service can determine if a Web server is available across different networks worldwide. The more the locations used, the more complete is the picture on network availability.\r\n\r\nWeb server monitoring process\r\nWhen monitoring a web server for potential problems, an external web monitoring service checks a number of parameters. First of all, it monitors for a proper HTTP return code. By HTTP specifications RFC 2616, any web server returns several HTTP codes. Analysis of the HTTP codes is the fastest way to determine the current status of the monitored web server. Third-party application performance monitoring tools provide additional web server monitoring, alerting and reporting capabilities.\r\n\r\nNotification\r\nAs the information brought by web server monitoring services is in most cases urgent and may be of crucial importance, various notification methods may be used: e-mail, land-line and cell phones, messengers, SMS, fax, pagers, etc.\r\n'),
(3, 'https://en.wikipedia.org/wiki/Service-level_agreement', '2019-12-03 13:46:18', 'Service-level agreement\r\nFrom Wikipedia, the free encyclopedia\r\nJump to navigationJump to search\r\nFor other uses, see SLA (disambiguation).\r\nA service-level agreement (SLA) is a commitment between a service provider and a client. Particular aspects of the service – quality, availability, responsibilities – are agreed between the service provider and the service user.[1] The most common component of an SLA is that the services should be provided to the customer as agreed upon in the contract. As an example, Internet service providers and telcos will commonly include service level agreements within the terms of their contracts with customers to define the level(s) of service being sold in plain language terms. In this case the SLA will typically have a technical definition in mean time between failures (MTBF), mean time to repair or mean time to recovery (MTTR); identifying which party is responsible for reporting faults or paying fees; responsibility for various data rates; throughput; jitter; or similar measurable details.\r\n\r\n\r\nContents\r\n1	Overview\r\n2	Components\r\n3	Common metrics\r\n4	Specific examples\r\n4.1	Backbone Internet providers\r\n4.2	WSLA\r\n4.3	Cloud computing\r\n4.4	Outsourcing\r\n5	See also\r\n6	References\r\n7	External links\r\nOverview\r\nA service-level agreement is an agreement between two or more parties, where one is the customer and the others are service providers. This can be a legally binding formal or an informal \"contract\" (for example, internal department relationships). The agreement may involve separate organizations, or different teams within one organization. Contracts between the service provider and other third parties are often (incorrectly) called SLAs – because the level of service has been set by the (principal) customer, there can be no \"agreement\" between third parties; these agreements are simply \"contracts.\" Operational-level agreements or OLAs, however, may be used by internal groups to support SLAs. If some aspect of a service has not been agreed with the customer, it is not an \"SLA\".\r\n\r\nSLAs commonly include many components, from a definition of services to the termination of agreement.[2] To ensure that SLAs are consistently met, these agreements are often designed with specific lines of demarcation and the parties involved are required to meet regularly to create an open forum for communication. Rewards and penalties applying to the provider are often specified. Most SLAs also leave room for periodic (annual) revisitation to make changes.[3]\r\n\r\nSince late 1980s SLAs have been used by fixed line telecom operators. SLAs are so widely used these days that larger organizations have many different SLAs existing within the company itself. Two different units in an organization script a SLA with one unit being the customer and another being the service provider. This practice helps to maintain the same quality of service amongst different units in the organization and also across multiple locations of the organization. This internal scripting of SLA also helps to compare the quality of service between an in-house department and an external service provider.[4]\r\n\r\nThe output received by the customer as a result of the service provided is the main focus of the service level agreement.\r\n\r\nService level agreements are also defined at different levels:\r\n\r\nCustomer-based SLA: An agreement with an individual customer group, covering all the services they use. For example, an SLA between a supplier (IT service provider) and the finance department of a large organization for the services such as finance system, payroll system, billing system, procurement/purchase system, etc.\r\nService-based SLA: An agreement for all customers using the services being delivered by the service provider. For example:\r\nA mobile service provider offers a routine service to all the customers and offers certain maintenance as a part of an offer with the universal charging.\r\nAn email system for the entire organization. There are chances of difficulties arising in this type of SLA as level of the services being offered may vary for different customers (for example, head office staff may use high-speed LAN connections while local offices may have to use a lower speed leased line).\r\nMultilevel SLA: The SLA is split into the different levels, each addressing different set of customers for the same services, in the same SLA.\r\nCorporate-level SLA: Covering all the generic service level management (often abbreviated as SLM) issues appropriate to every customer throughout the organization. These issues are likely to be less volatile and so updates (SLA reviews) are less frequently required.\r\nCustomer-level SLA: covering all SLM issues relevant to the particular customer group, regardless of the services being used.\r\nService-level SLA: covering all SLM issue relevant to the specific services, in relation to this specific customer group.\r\nComponents\r\nA well defined and typical SLA will contain the following components:[5]\r\n\r\nType of service to be provided: It specifies the type of service and any additional details of type of service to be provided. In case of an IP network connectivity, type of service will describe functions such as operation and maintenance of networking equipment, connection bandwidth to be provided, etc.\r\nThe service\'s desired performance level, especially its reliability and responsiveness: A reliable service will be the one which suffers minimum disruptions in a specific amount of time and is available at almost all times. A service with good responsiveness will perform the desired action promptly after the customer requests for it.\r\nMonitoring process and service level reporting: This component describes how the performance levels are supervised and monitored. This process involves gathering of different type of statistics, how frequently this statistics will be collected and how this statistics will be accessed by the customers.\r\nThe steps for reporting issues with the service: This component will specify the contact details to report the problem to and the order in which details about the issue have to be reported. The contract will also include a time range in which the problem will be looked upon and also till when the issue will be resolved.\r\nResponse and issue resolution time-frame: Response time-frame is the time period by which the service provider will start the investigation of the issue. Issue resolution time-frame is the time period by which the current service issue will be resolved and fixed.\r\nRepercussions for service provider not meeting its commitment: If the provider is not able to meet the requirements as stated in SLA then service provider will have to face consequences for the same. These consequences may include customer\'s right to terminate the contract or ask for a refund for losses incurred by the customer due to failure of service.\r\nCommon metrics\r\nService-level agreements can contain numerous service-performance metrics with corresponding service-level objectives. A common case in IT-service management is a call center or service desk. Metrics commonly agreed to in these cases include:\r\n\r\nAbandonment Rate: Percentage of calls abandoned while waiting to be answered.\r\nASA (Average Speed to Answer): Average time (usually in seconds) it takes for a call to be answered by the service desk.\r\nTSF (Time Service Factor): Percentage of calls answered within a definite timeframe, e.g., 80% in 20 seconds.\r\nFCR (First-Call Resolution): Percentage of incoming calls that can be resolved without the use of a callback or without having the caller call back the helpdesk to finish resolving the case.[6]\r\nTAT (Turn-Around Time): Time taken to complete a certain task.\r\nTRT (total resolution time): Total time taken to complete a certain task.\r\nMTTR (Mean Time To Recover): Time taken to recover after an outage of service.\r\nUptime is also a common metric, often used for data services such as shared hosting, virtual private servers and dedicated servers. Common agreements include percentage of network uptime, power uptime, number of scheduled maintenance windows, etc.\r\n\r\nMany SLAs track to the Information Technology Infrastructure Library specifications when applied to IT services.\r\n\r\nSpecific examples\r\nBackbone Internet providers\r\nIt is not uncommon for an internet backbone service provider (or network service provider) to explicitly state its own SLA on its website.[7][8][9] The U.S. Telecommunications Act of 1996 does not expressly mandate that companies have SLAs, but it does provide a framework for firms to do so in Sections 251 and 252.[10] Section 252(c)(1) for example (\"Duty to Negotiate\") requires Incumbent local exchange carriers (ILECs) to negotiate in good faith about matters such as resale and access to rights of way.\r\n\r\nWSLA\r\nA web service level agreement (WSLA) is a standard for service level agreement compliance monitoring of web services. It allows authors to specify the performance metrics associated with a web service application, desired performance targets, and actions that should be performed when performance is not met.\r\n\r\nWSLA Language Specification, version 1.0 was published by IBM on January 28, 2001.\r\n\r\nCloud computing\r\nThe underlying benefit of cloud computing is shared resources, which is supported by the underlying nature of a shared infrastructure environment. Thus, SLAs span across the cloud and are offered by service providers as a service-based agreement rather than a customer-based agreement. Measuring, monitoring and reporting on cloud performance is based on the end UX or their ability to consume resources. The downside of cloud computing relative to SLAs is the difficulty in determining the root cause of service interruptions due to the complex nature of the environment.\r\n\r\nAs applications are moved from dedicated hardware into the cloud, they need to achieve the same or even more demanding levels of service than classical installations. SLAs for cloud services focus on characteristics of the data center and more recently include characteristics of the network (see carrier cloud) to support end-to-end SLAs.[11]\r\n\r\nAny SLA management strategy considers two well-differentiated phases: negotiating the contract and monitoring its fulfilment in real time. Thus, SLA management encompasses the SLA contract definition: the basic schema with the QoS parameters; SLA negotiation; SLA monitoring; SLA violation detection; and SLA enforcement—according to defined policies.\r\n\r\nThe main point is to build a new layer upon the grid, cloud, or SOA middleware able to create a negotiation mechanism between the providers and consumers of services. An example is the EU–funded Framework 7 research project, SLA@SOI,[12] which is researching aspects of multi-level, multi-provider SLAs within service-oriented infrastructure and cloud computing, while another EU-funded project, VISION Cloud,[13] has provided results with respect to content-oriented SLAs.\r\n\r\nFP7 IRMOS also investigated aspects of translating application-level SLA terms to resource-based attributes in an effort to bridge the gap between client-side expectations and cloud-provider resource-management mechanisms.[14][15] A summary of the results of various research projects in the area of SLAs (ranging from specifications to monitoring, management and enforcement) has been provided by the European Commission.[16]\r\n\r\nOutsourcing\r\nOutsourcing involves the transfer of responsibility from an organization to a supplier. This new arrangement is managed through a contract that may include one or more SLAs. The contract may involve financial penalties and the right to terminate if any of the SLAs metrics are consistently missed. Setting, tracking and managing SLAs is an important part of the outsourcing relationship management (ORM) discipline. Specific SLAs are typically negotiated up front as part of the outsourcing contract and used as one of the primary tools of outsourcing governance.\r\n\r\nIn software development, specific SLAs can apply to application outsourcing contracts in line with standards in software quality, as well as recommendations provided by neutral organizations like CISQ, which has published numerous papers on the topic (such as Using Software Measurement in SLAs[17]) that are available to the public.'),
(4, 'https://en.wikipedia.org/wiki/High_availability', '2019-12-03 13:46:18', 'High availability\r\nFrom Wikipedia, the free encyclopedia\r\nJump to navigationJump to search\r\n\"Always-on\" redirects here. For the software restriction, see Always-on DRM.\r\nHigh availability (HA) is a characteristic of a system, which aims to ensure an agreed level of operational performance, usually uptime, for a higher than normal period.\r\n\r\nModernization has resulted in an increased reliance on these systems. For example, hospitals and data centers require high availability of their systems to perform routine daily activities. Availability refers to the ability of the user community to obtain a service or good, access the system, whether to submit new work, update or alter existing work, or collect the results of previous work. If a user cannot access the system, it is – from the users point of view – unavailable.[1] Generally, the term downtime is used to refer to periods when a system is unavailable.\r\n\r\n\r\nContents\r\n1	Principles\r\n2	Scheduled and unscheduled downtime\r\n3	Percentage calculation\r\n3.1	\"Nines\"\r\n4	Measurement and interpretation\r\n5	Closely related concepts\r\n6	Military control systems\r\n7	System design\r\n8	Reasons for unavailability\r\n9	Costs of unavailability\r\n10	See also\r\n11	Notes\r\n12	References\r\n13	External links\r\nPrinciples\r\nThere are three principles of systems design in reliability engineering which can help achieve high availability.\r\n\r\nElimination of single points of failure. This means adding redundancy to the system so that failure of a component does not mean failure of the entire system.\r\nReliable crossover. In redundant systems, the crossover point itself tends to become a single point of failure. Reliable systems must provide for reliable crossover.\r\nDetection of failures as they occur. If the two principles above are observed, then a user may never see a failure – but the maintenance activity must.\r\nScheduled and unscheduled downtime\r\n\r\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed.\r\nFind sources: \"High availability\" – news · newspapers · books · scholar · JSTOR (June 2008) (Learn how and when to remove this template message)\r\nA distinction can be made between scheduled and unscheduled downtime. Typically, scheduled downtime is a result of maintenance that is disruptive to system operation and usually cannot be avoided with a currently installed system design. Scheduled downtime events might include patches to system software that require a reboot or system configuration changes that only take effect upon a reboot. In general, scheduled downtime is usually the result of some logical, management-initiated event. Unscheduled downtime events typically arise from some physical event, such as a hardware or software failure or environmental anomaly. Examples of unscheduled downtime events include power outages, failed CPU or RAM components (or possibly other failed hardware components), an over-temperature related shutdown, logically or physically severed network connections, security breaches, or various application, middleware, and operating system failures.\r\n\r\nIf users can be warned away from scheduled downtimes, then the distinction is useful. But if the requirement is for true high availability, then downtime is downtime whether or not it is scheduled.\r\n\r\nMany computing sites exclude scheduled downtime from availability calculations, assuming that it has little or no impact upon the computing user community. By doing this, they can claim to have phenomenally high availability, which might give the illusion of continuous availability. Systems that exhibit truly continuous availability are comparatively rare and higher priced, and most have carefully implemented specialty designs that eliminate any single point of failure and allow online hardware, network, operating system, middleware, and application upgrades, patches, and replacements. For certain systems, scheduled downtime does not matter, for example system downtime at an office building after everybody has gone home for the night.\r\n\r\nPercentage calculation\r\nAvailability is usually expressed as a percentage of uptime in a given year. The following table shows the downtime that will be allowed for a particular percentage of availability, presuming that the system is required to operate continuously. Service level agreements often refer to monthly downtime or availability in order to calculate service credits to match monthly billing cycles. The following table shows the translation from a given availability percentage to the corresponding amount of time a system would be unavailable.\r\n\r\nAvailability %	Downtime per year[note 1]	Downtime per month	Downtime per week	Downtime per day\r\n55.5555555% (\"nine fives\")	162.33 days	13.53 days	74.92 hours	10.67 hours\r\n90% (\"one nine\")	36.53 days	73.05 hours	16.80 hours	2.40 hours\r\n95% (\"one nine five\")	18.26 days	36.53 hours	8.40 hours	1.20 hours\r\n97%	10.96 days	21.92 hours	5.04 hours	43.20 minutes\r\n98%	7.31 days	14.61 hours	3.36 hours	28.80 minutes\r\n99% (\"two nines\")	3.65 days	7.31 hours	1.68 hours	14.40 minutes\r\n99.5% (\"two nines five\")	1.83 days	3.65 hours	50.40 minutes	7.20 minutes\r\n99.8%	17.53 hours	87.66 minutes	20.16 minutes	2.88 minutes\r\n99.9% (\"three nines\")	8.77 hours	43.83 minutes	10.08 minutes	1.44 minutes\r\n99.95% (\"three nines five\")	4.38 hours	21.92 minutes	5.04 minutes	43.20 seconds\r\n99.99% (\"four nines\")	52.60 minutes	4.38 minutes	1.01 minutes	8.64 seconds\r\n99.995% (\"four nines five\")	26.30 minutes	2.19 minutes	30.24 seconds	4.32 seconds\r\n99.999% (\"five nines\")	5.26 minutes	26.30 seconds	6.05 seconds	864.00 milliseconds\r\n99.9999% (\"six nines\")	31.56 seconds	2.63 seconds	604.80 milliseconds	86.40 milliseconds\r\n99.99999% (\"seven nines\")	3.16 seconds	262.98 milliseconds	60.48 milliseconds	8.64 milliseconds\r\n99.999999% (\"eight nines\")	315.58 milliseconds	26.30 milliseconds	6.05 milliseconds	864.00 microseconds\r\n99.9999999% (\"nine nines\")	31.56 milliseconds	2.63 milliseconds	604.80 microseconds	86.40 microseconds\r\nUptime and availability can be used synonymously as long as the items being discussed are kept consistent. That is, a system can be up, but its services are not available, as in the case of a network outage. This can also be viewed as a system that is available to be worked on, but its services are not up from a functional perspective (as opposed to software service/process perspective). The perspective is important here - whether the item being discussed is the server hardware, server OS, functional service, software service/process...etc. Keep the perspective consistent throughout a discussion, then uptime and availability can be used synonymously.\r\n\r\n\"Nines\"\r\nSee also: List of unusual units of measurement § Nines\r\nPercentages of a particular order of magnitude are sometimes referred to by the number of nines or \"class of nines\" in the digits. For example, electricity that is delivered without interruptions (blackouts, brownouts or surges) 99.999% of the time would have 5 nines reliability, or class five.[2] In particular, the term is used in connection with mainframes[3][4] or enterprise computing, often as part of a service-level agreement.\r\n\r\nSimilarly, percentages ending in a 5 have conventional names, traditionally the number of nines, then \"five\", so 99.95% is \"three nines five\", abbreviated 3N5.[5][6] This is casually referred to as \"three and a half nines\",[7] but this is incorrect: a 5 is only a factor of 2, while a 9 is a factor of 10, so a 5 is 0.3 nines (per below formula: {\\displaystyle \\log _{10}2\\approx 0.3}\\log _{{10}}2\\approx 0.3):[note 2] 99.95% availability is 3.3 nines, not 3.5 nines.[8] More simply, going from 99.9% availability to 99.95% availability is a factor of 2 (0.1% to 0.05% unavailability), but going from 99.95% to 99.99% availability is a factor of 5 (0.05% to 0.01% unavailability), over twice as much.[note 3]\r\n\r\nA formulation of the class of 9s {\\displaystyle c}c based on a system\'s unavailability {\\displaystyle x}x would be\r\n\r\n{\\displaystyle c:=\\lfloor -\\log _{10}x\\rfloor }c:=\\lfloor -\\log _{10}x\\rfloor\r\n(cf. Floor and ceiling functions).\r\n\r\nA similar measurement is sometimes used to describe the purity of substances.\r\n\r\nIn general, the number of nines is not often used by a network engineer when modeling and measuring availability because it is hard to apply in formula. More often, the unavailability expressed as a probability (like 0.00001), or a downtime per year is quoted. Availability specified as a number of nines is often seen in marketing documents.[citation needed] The use of the \"nines\" has been called into question, since it does not appropriately reflect that the impact of unavailability varies with its time of occurrence.[9] For large amounts of 9s, the \"unavailability\" index (measure of downtime rather than uptime) is easier to handle. For example, this is why an \"unavailability\" rather than availability metric is used in hard disk or data link bit error rates.\r\n\r\nMeasurement and interpretation\r\nAvailability measurement is subject to some degree of interpretation. A system that has been up for 365 days in a non-leap year might have been eclipsed by a network failure that lasted for 9 hours during a peak usage period; the user community will see the system as unavailable, whereas the system administrator will claim 100% uptime. However, given the true definition of availability, the system will be approximately 99.9% available, or three nines (8751 hours of available time out of 8760 hours per non-leap year). Also, systems experiencing performance problems are often deemed partially or entirely unavailable by users, even when the systems are continuing to function. Similarly, unavailability of select application functions might go unnoticed by administrators yet be devastating to users — a true availability measure is holistic.\r\n\r\nAvailability must be measured to be determined, ideally with comprehensive monitoring tools (\"instrumentation\") that are themselves highly available. If there is a lack of instrumentation, systems supporting high volume transaction processing throughout the day and night, such as credit card processing systems or telephone switches, are often inherently better monitored, at least by the users themselves, than systems which experience periodic lulls in demand.\r\n\r\nAn alternative metric is mean time between failures (MTBF).\r\n\r\nClosely related concepts\r\nRecovery time (or estimated time of repair (ETR), also known as recovery time objective (RTO) is closely related to availability, that is the total time required for a planned outage or the time required to fully recover from an unplanned outage. Another metric is mean time to recovery (MTTR). Recovery time could be infinite with certain system designs and failures, i.e. full recovery is impossible. One such example is a fire or flood that destroys a data center and its systems when there is no secondary disaster recovery data center.\r\n\r\nAnother related concept is data availability, that is the degree to which databases and other information storage systems faithfully record and report system transactions. Information management often focuses separately on data availability, or Recovery Point Objective, in order to determine acceptable (or actual) data loss with various failure events. Some users can tolerate application service interruptions but cannot tolerate data loss.\r\n\r\nA service level agreement (\"SLA\") formalizes an organization\'s availability objectives and requirements.\r\n\r\nMilitary control systems\r\nHigh availability is one of the primary requirements of the control systems in unmanned vehicles and autonomous maritime vessels. If the controlling system becomes unavailable, the Ground Combat Vehicle (GCV) or ASW Continuous Trail Unmanned Vessel (ACTUV) would be lost.\r\n\r\nSystem design\r\nAdding more components to an overall system design can undermine efforts to achieve high availability because complex systems inherently have more potential failure points and are more difficult to implement correctly. While some analysts would put forth the theory that the most highly available systems adhere to a simple architecture (a single, high quality, multi-purpose physical system with comprehensive internal hardware redundancy), this architecture suffers from the requirement that the entire system must be brought down for patching and operating system upgrades. More advanced system designs allow for systems to be patched and upgraded without compromising service availability (see load balancing and failover).\r\n\r\nHigh availability requires less human intervention to restore operation in complex systems; the reason for this being that the most common cause for outages is human error.[10]\r\n\r\nRedundancy is used to create systems with high levels of availability (e.g. aircraft flight computers). In this case it is required to have high levels of failure detectability and avoidance of common cause failures. Two kinds of redundancy are passive redundancy and active redundancy.\r\n\r\nPassive redundancy is used to achieve high availability by including enough excess capacity in the design to accommodate a performance decline. The simplest example is a boat with two separate engines driving two separate propellers. The boat continues toward its destination despite failure of a single engine or propeller. A more complex example is multiple redundant power generation facilities within a large system involving electric power transmission. Malfunction of single components is not considered to be a failure unless the resulting performance decline exceeds the specification limits for the entire system.\r\n\r\nActive redundancy is used in complex systems to achieve high availability with no performance decline. Multiple items of the same kind are incorporated into a design that includes a method to detect failure and automatically reconfigure the system to bypass failed items using a voting scheme. This is used with complex computing systems that are linked. Internet routing is derived from early work by Birman and Joseph in this area.[11] Active redundancy may introduce more complex failure modes into a system, such as continuous system reconfiguration due to faulty voting logic.\r\n\r\nZero downtime system design means that modeling and simulation indicates mean time between failures significantly exceeds the period of time between planned maintenance, upgrade events, or system lifetime. Zero downtime involves massive redundancy, which is needed for some types of aircraft and for most kinds of communications satellites. Global Positioning System is an example of a zero downtime system.\r\n\r\nFault instrumentation can be used in systems with limited redundancy to achieve high availability. Maintenance actions occur during brief periods of down-time only after a fault indicator activates. Failure is only significant if this occurs during a mission critical period.\r\n\r\nModeling and simulation is used to evaluate the theoretical reliability for large systems. The outcome of this kind of model is used to evaluate different design options. A model of the entire system is created, and the model is stressed by removing components. Redundancy simulation involves the N-x criteria. N represents the total number of components in the system. x is the number of components used to stress the system. N-1 means the model is stressed by evaluating performance with all possible combinations where one component is faulted. N-2 means the model is stressed by evaluating performance with all possible combinations where two component are faulted simultaneously.\r\n\r\nReasons for unavailability\r\nA survey among academic availability experts in 2010 ranked reasons for unavailability of enterprise IT systems. All reasons refer to not following best practice in each of the following areas (in order of importance):[12]\r\n\r\nMonitoring of the relevant components\r\nRequirements and procurement\r\nOperations\r\nAvoidance of network failures\r\nAvoidance of internal application failures\r\nAvoidance of external services that fail\r\nPhysical environment\r\nNetwork redundancy\r\nTechnical solution of backup\r\nProcess solution of backup\r\nPhysical location\r\nInfrastructure redundancy\r\nStorage architecture redundancy\r\nA book on the factors themselves was published in 2003.[13]\r\n\r\nCosts of unavailability\r\nIn a 1998 report from IBM Global Services, unavailable systems were estimated to have cost American businesses $4.54 billion in 1996, due to lost productivity and revenues.[14]'),
(5, 'https://en.wikipedia.org/wiki/Network_traffic_measurement', '2019-12-03 13:46:50', 'Network traffic measurement\r\nFrom Wikipedia, the free encyclopedia\r\nJump to navigationJump to search\r\nIn computer networks, network traffic measurement is the process of measuring the amount and type of traffic on a particular network. This is especially important with regard to effective bandwidth management.\r\n\r\n\r\nContents\r\n1	Techniques\r\n2	Measurement studies\r\n3	Tools\r\n3.1	Functions and features\r\n4	See also\r\n5	References\r\n6	External links\r\nTechniques\r\nNetwork performance could be measured using either active or passive techniques. Active techniques (e.g. Iperf) are more intrusive but are arguably more accurate. Passive techniques have less network overhead and hence can run in the background to be used to trigger network management actions.\r\n\r\nMeasurement studies\r\nA range of studies have been performed from various points on the Internet. The AMS-IX (Amsterdam Internet Exchange) is one of the world\'s largest Internet exchanges. It produces a constant supply of simple Internet statistics. There are also numerous academic studies that have produced a range of measurement studies[1][2][3] on frame size distributions, TCP/UDP ratios and TCP/IP options.\r\n\r\nTools\r\nVarious software tools are available to measure network traffic. Some tools measure traffic by sniffing and others use SNMP, WMI or other local agents to measure bandwidth use on individual machines and routers. However, the latter generally do not detect the type of traffic, nor do they work for machines which are not running the necessary agent software, such as rogue machines on the network, or machines for which no compatible agent is available. In the latter case, inline appliances are preferred. These would generally \'sit\' between the LAN and the LAN\'s exit point, generally the WAN or Internet router, and all packets leaving and entering the network would go through them. In most cases the appliance would operate as a bridge on the network so that it is undetectable by users.\r\n\r\nSome tools used for SNMP monitoring are InfoVista[4], Tivoli Netcool/Proviso [5] by IBM, CA Performance Management by CA Technologies[6], TotalView [7] by PathSolutions, SolarWinds[8], and NMIS from Opmantek.\r\n\r\nFunctions and features\r\nMeasurement tools generally have these functions and features:\r\n\r\nUser interface (web, graphical, console)\r\nReal-time traffic graphs\r\nNetwork activity is often reported against pre-configured traffic matching rules to show:\r\nLocal IP address\r\nRemote IP address\r\nPort number or protocol\r\nLogged in user name\r\nBandwidth quotas\r\nSupport for traffic shaping or rate limiting (overlapping with the network traffic control page)\r\nSupport website blocking and content filtering\r\nAlarms to notify the administrator of excessive usage (by IP address or in total)\r\nSee also\r\nIP Flow Information Export and NetFlow\r\nMeasuring network throughput\r\nNetwork management\r\nNetwork monitoring\r\nNetwork scheduler\r\nNetwork simulation\r\nPacket sniffer\r\nPerformance management\r\nReferences\r\n Murray, David; Terry Koziniec (2012). \"The State of Enterprise Network Traffic in 2012\". 18th Asia-Pacific Conference on Communications (APCC 2012).\r\n Zhang, Min; Maurizio Dusi; Wolfgang John; Changjia Chen (2009). \"Analysis of udp traffic usage on internet backbone links\". In Proceedings of the 2009 Ninth Annual International Symposium on Applications and the Internet.\r\n Wolfgang, John; Sven Tafvelin (2007). \"Analysis of internet backbone traffic and header anomalies observed\". ACM Wireless Networks. Proceedings of the 7th ACM SIGCOMM conference on Internet measurement.\r\n \"Product Overview\". InfoVista.com. Retrieved 27 September 2018.\r\n \"Configuring IBM Tivoli Storage Manager SNMP\". ibm.com. Retrieved 27 September 2018.\r\n \"CA Performance Management - 2.8\". docops.ca.com. Retrieved 27 September 2018.\r\n \"Selecting The Right Network Troubleshooting Tool Part Three: SNMP\". PathSolutions.com. Retrieved 27 September 2018.\r\n \"SNMP Monitoring\". SolarWinds.com. Retrieved 27 September 2018.\r\n');

